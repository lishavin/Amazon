{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9Fk3GfVH3a3G",
        "5f8dP3615L2p",
        "MJxACWVIQEF8",
        "pUC59DTrRhYo",
        "W2y-gVxQfCjB",
        "Oz5UM2DLam6X",
        "ixnYmNLXO_LX",
        "uk-Ujcg7QKXr",
        "w7QloeVyS91Z",
        "3263Yn7NfsuV",
        "AlPrlqckhIcY",
        "Ge7uZX1Pj6I4",
        "-XFEIGAOknUo",
        "fXhv4fCbk69M",
        "vbdAaR3JojvY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lishavin/Amazon/blob/main/TF-IDF%2BLogistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Required Packages\n",
        "\n",
        "- íŒŒì¼ > Driveì— ì‚¬ë³¸ ì €ì¥\n",
        "- ì•„ë˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í›„ session restart í•„ìš” (ëŸ°íƒ€ì„ > ì„¸ì…˜ë‹¤ì‹œì‹œì‘)\n",
        "- drive mount (pretrained weight)"
      ],
      "metadata": {
        "id": "sENqptvHUisT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install numpy==1.26\n",
        "!pip install scipy==1.13.1\n",
        "!pip install gensim\n",
        "!pip install fsspec==2023.4.0 #\"**\" ê²½ë¡œ íŒ¨í„´ í˜¸í™˜ë˜ëŠ” fsspec ì˜›ë‚ ë²„ì „ìœ¼ë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ\n",
        "\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "G9aLq4ZATesT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_ilO50MAuNUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SSipMlOTUml"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "import nltk #í† í°í™”, ë¶ˆìš©ì–´ ì œê±°, í‘œì œì–´ ì¶”ì¶œ ë“±ì„ ìœ„í•œ ìì› ë‹¤ìš´ë¡œë“œ\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Data"
      ],
      "metadata": {
        "id": "joSdnEARUlDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "df = load_dataset(\"fancyzhx/amazon_polarity\")\n",
        "\n",
        "# trainê³¼ testë¥¼ ê°ê° pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
        "train_df = df[\"train\"].select(range(10000)).to_pandas()\n",
        "test_df = df[\"test\"].select(range(5000)).to_pandas()"
      ],
      "metadata": {
        "id": "sLPn7NAZTb6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_df)\n",
        "type(test_df)"
      ],
      "metadata": {
        "id": "52lgghqYIlo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. EDA\n"
      ],
      "metadata": {
        "id": "xSo0G0RAUmbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1 : positive\n",
        "- 0 : negative"
      ],
      "metadata": {
        "id": "9Wx9q1DJSxBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "DGqxjaUDWeAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data type í™•ì¸\n",
        "print(\"Train Dataset DataType\")\n",
        "print(train_df.dtypes)\n",
        "\n",
        "print(\"\\nTest Dataset DataType\")\n",
        "print(test_df.dtypes)"
      ],
      "metadata": {
        "id": "tWzrTcd7WtFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isnull()"
      ],
      "metadata": {
        "id": "OqNMb3m0JFUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ê²°ì¸¡ì¹˜ í™•ì¸\n",
        "print(\"Train Dataset ê²°ì¸¡ì¹˜\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(\"\\nTest Dataset ê²°ì¸¡ì¹˜\")\n",
        "print(test_df.isnull().sum())"
      ],
      "metadata": {
        "id": "1sjzijn4WyBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label ë¹„ìœ¨ í™•ì¸\n",
        "import seaborn as sns\n",
        "sns.countplot(x='label', data= train_df)\n",
        "print(train_df.label.value_counts())"
      ],
      "metadata": {
        "id": "nIOr90GaXJ6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label ë¹„ìœ¨ í™•ì¸\n",
        "sns.countplot(x='label', data= test_df)\n",
        "print(test_df.label.value_counts())"
      ],
      "metadata": {
        "id": "UL-Jz5lUXZZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "2B_vvPTeXzaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Text Cleaning"
      ],
      "metadata": {
        "id": "9Fk3GfVH3a3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">1. ì •ê·œí‘œí˜„ì‹\n",
        "  - HTML íƒœê·¸ ì œê±°\n",
        "  - íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "2. ì†Œë¬¸ì ë³€í™˜\n",
        "3. Stopwords ì œê±°\n",
        "4. Stemming (ì–´ê°„ ì¶”ì¶œ) /Lemmatization (í‘œì œì–´ ì¶”ì¶œ)\n",
        "\n"
      ],
      "metadata": {
        "id": "hSV-0HR3Nbe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Amazon ë°ì´í„°ì— ì ìš©"
      ],
      "metadata": {
        "id": "TmqEjPLQMrub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# titleê³¼ content ì—´ í•©ì¹˜ê¸°\n",
        "train_df[\"review\"] = train_df[\"title\"] + \" \" + train_df[\"content\"]\n",
        "test_df[\"review\"] = test_df[\"title\"] + \" \" + test_df[\"content\"]"
      ],
      "metadata": {
        "id": "E5uczMGdOhco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re #ì •ê·œí‘œí˜„ì‹ regular expression: HTML íƒœê·¸ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì œê±°ì‹œ ì‚¬ìš©\n",
        "import nltk\n",
        "from nltk.corpus import stopwords #nltkì—ì„œ ì œê³µí•˜ëŠ” ë¶ˆìš©ì–´(stopwords) ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ê²ƒ\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer #ì–´ê°„ ì¶”ì¶œ, í‘œì œì–´ ì¶”ì¶œ\n",
        "\n",
        "# ì²˜ìŒ í•œ ë²ˆì€ ë‹¤ìš´ë¡œë“œ í•„ìš”\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_review(review):\n",
        "    # HTML íƒœê·¸ ì œê±°\n",
        "    review = re.sub('<[^>]*>', '', review)\n",
        "    # íŠ¹ìˆ˜ ë¬¸ì ì œê±° (!ëŠ” ë‚¨ê¸°ê¸°)\n",
        "    review = re.sub('[^a-zA-Z0-9 ?]', '', review)\n",
        "    # ì†Œë¬¸ì ë³€í™˜\n",
        "    review = review.lower()\n",
        "    # í† í°í™”\n",
        "    tokens = word_tokenize(review)\n",
        "    # ë¶ˆìš©ì–´ ì œê±°\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # í‘œì œì–´ ì¶”ì¶œ\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "xZmPGCKm31ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ ì ìš©\n",
        "train_df['processed_review'] = train_df['review'].apply(preprocess_review)\n",
        "test_df['processed_review'] = test_df['review'].apply(preprocess_review)"
      ],
      "metadata": {
        "id": "Zx4GvXa-48xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "f-MxJ-AparoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train-Valid Split"
      ],
      "metadata": {
        "id": "W2y-gVxQfCjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80% train, 20% validation\n",
        "data_train, data_valid = train_test_split(\n",
        "    train_df,                  # ì›ë˜ train ë°ì´í„°ì…‹\n",
        "    test_size=0.2,             # 20%ëŠ” validationìœ¼ë¡œ\n",
        "    stratify=train_df['label'], # label ë¹„ìœ¨ ìœ ì§€ (ê¸/ë¶€ì • ê· í˜•)\n",
        "    random_state=42           # ì¬í˜„ì„± (ê°™ì€ split ê²°ê³¼)\n",
        ")\n",
        "\n",
        "data_test = test_df           # í…ŒìŠ¤íŠ¸ëŠ” ì´ë¯¸ ë³„ë„ë¡œ ìˆìŒ"
      ],
      "metadata": {
        "id": "zd5S9b8pfKkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_train)"
      ],
      "metadata": {
        "id": "Mku9ygoRMeXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_valid)"
      ],
      "metadata": {
        "id": "rLmnGrAyMf5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Vectorization + Classifier"
      ],
      "metadata": {
        "id": "Oz5UM2DLam6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **TF-IDF Vectorizer**"
      ],
      "metadata": {
        "id": "uk-Ujcg7QKXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF ë²¡í„°í™” (ìµœëŒ€ 10000ê°œì˜ ë‹¨ì–´ ì‚¬ìš©)\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "# í•™ìŠµ ë°ì´í„°ë¡œ í•™ìŠµ + ë³€í™˜\n",
        "X_train = vectorizer.fit_transform(data_train['processed_review'])\n",
        "X_valid = vectorizer.transform(data_valid['processed_review'])\n",
        "X_test = vectorizer.transform(data_test['processed_review'])\n",
        "\n",
        "# ê°ì„± ë¶„ë¥˜ì˜ ì •ë‹µ(label)ê°’ì„ ë”°ë¡œ ë¶„ë¦¬í•´ì„œ ì €ì¥\n",
        "y_train = data_train[\"label\"]\n",
        "y_valid = data_valid[\"label\"]\n",
        "y_test = data_test[\"label\"]"
      ],
      "metadata": {
        "id": "TVj_smEPQIPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ë²¡í„° í™•ì¸\n",
        "print(X_train[0].toarray()[0][170:250])\n",
        "print(len(X_train[0].toarray()[0])) # ì´ ë²¡í„° ê¸¸ì´: 10,000"
      ],
      "metadata": {
        "id": "eEX0MfGPSV-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0ì´ ì•„ë‹Œ í•­ëª© ìˆ˜ í™•ì¸: 14ê°œë¡œ ì˜ ë‚˜ì˜´\n",
        "nonzero_count = (X_train[0].toarray() != 0).sum()\n",
        "print(\"Non-zero TF-IDF features:\", nonzero_count)"
      ],
      "metadata": {
        "id": "HouypEjxMeO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Logistic Regression**"
      ],
      "metadata": {
        "id": "vbdAaR3JojvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ê²°ê³¼ì— ëŒ€í•œ í™•ë¥ ê°’ ì¶œë ¥ ê°€ëŠ¥\n",
        "- ì„ í˜• ê²°ì • ê²½ê³„ë§Œ í•™ìŠµ ê°€ëŠ¥ (ë³µì¡í•œ íŒ¨í„´ì€ í•œê³„ê°€ ìˆìŒ) <br>\n",
        "[ì°¸ê³ docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ],
      "metadata": {
        "id": "m9URfTv2opYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "#model load\n",
        "log_clf = LogisticRegression(max_iter=1000)\n",
        "\n",
        "#train\n",
        "log_clf.fit(X_train, y_train)\n",
        "\n",
        "#prediction\n",
        "lr_valid_preds = log_clf.predict(X_valid)\n",
        "lr_test_preds = log_clf.predict(X_test)\n",
        "\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_valid, lr_valid_preds))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, lr_test_preds))\n",
        "print(\"\\n[Classification Report on Test Set]\\n\", classification_report(y_test, lr_test_preds))"
      ],
      "metadata": {
        "id": "eowMa0OapJWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation 1\n",
        "K-fold ì„ íƒ: íŠœë‹ ì „ baseline ì„±ëŠ¥ì„ í™•ì¸\n"
      ],
      "metadata": {
        "id": "N67aE9ZIopxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_accuracies = []  # ì •í™•ë„ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ ì„ ì–¸\n",
        "y_train = y_train.values  # ë˜ëŠ” y_train = np.array(y_train)\n",
        "\n",
        "# ê·¸ ë‹¤ìŒ K-Fold ì½”ë“œ ì •ìƒ ì‘ë™\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(X_train)):\n",
        "    X_tr, X_val = X_train[train_index], X_train[valid_index]\n",
        "    y_tr, y_val = y_train[train_index], y_train[valid_index]\n",
        "\n",
        "    log_clf.fit(X_tr, y_tr)\n",
        "    val_preds = log_clf.predict(X_val)\n",
        "    acc = accuracy_score(y_val, val_preds)\n",
        "    fold_accuracies.append(acc)\n",
        "\n",
        "    print(f\"[Pre-Tuning Fold {fold+1}] Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nğŸ” Average Validation Accuracy (Pre-Tuning 5-Fold): {np.mean(fold_accuracies):.4f}\")"
      ],
      "metadata": {
        "id": "DvCGpgTN5lGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "ì´ë¯¸ ëª¨ë¸ ì„±ëŠ¥ì´ ì¢‹ì•„ì„œ ì •ë°€í•œ ì „ìˆ˜ì¡°ì‚¬(Grid)ë³´ë‹¤ëŠ” \të¹ ë¥´ê²Œ ê·¼ì‚¬ ìµœì ê°’ì„ ì°¾ëŠ” ê²Œ ë” ì¤‘ìš”í•´ì„œ Random Searchë¡œ ìµœì  C, penalty ê°’ ì°¾ê¸°\n"
      ],
      "metadata": {
        "id": "LtES2bAOn33U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RandomizedSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import uniform\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •\n",
        "param_dist = {\n",
        "    'C': uniform(loc=0.001, scale=10),  # 0.001 ~ 10 ì‚¬ì´ ì—°ì†ê°’\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # l1ì„ ì§€ì›í•˜ëŠ” solver\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV ì •ì˜\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=LogisticRegression(max_iter=1000),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,                  # 20ê°œì˜ ì¡°í•©ì„ ë¬´ì‘ìœ„ë¡œ ì‹œë„\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    n_jobs=-1                  # ê°€ëŠ¥í•œ ëª¨ë“  CPU ì‚¬ìš©\n",
        ")\n",
        "\n",
        "# í•™ìŠµ\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", random_search.best_score_)"
      ],
      "metadata": {
        "id": "aqrBJk2coKjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation 2\n",
        "íŠœë‹ í›„ ìµœì¢… ëª¨ë¸ì„ ì¬ê²€ì¦: RandomizedSearch ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ê°œì„ ëëŠ”ì§€ ë¹„êµ"
      ],
      "metadata": {
        "id": "qoOBEnIhtYnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomizedSearchCV ì´í›„ ìµœì  ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# íŠœë‹ í›„ K-Fold ì¬ì„¤ì •\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "tuned_fold_accuracies = []\n",
        "\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(X_train)):\n",
        "    X_tr, X_val = X_train[train_index], X_train[valid_index]\n",
        "    y_tr, y_val = y_train[train_index], y_train[valid_index]\n",
        "\n",
        "    best_model.fit(X_tr, y_tr)\n",
        "    val_preds = best_model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, val_preds)\n",
        "    tuned_fold_accuracies.append(acc)\n",
        "\n",
        "    print(f\"[Post-Tuning Fold {fold+1}] Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nâœ… Average Validation Accuracy (Post-Tuning 5-Fold): {np.mean(tuned_fold_accuracies):.4f}\")"
      ],
      "metadata": {
        "id": "m-WexM-dteQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì¶”ê°€ ë¶„ì„\n"
      ],
      "metadata": {
        "id": "-N2QwHqHgt2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ê°ì„±ì— ì˜í–¥ì„ ì£¼ëŠ” ì£¼ìš” ë‹¨ì–´ ë¶„ì„ (model.coef_)"
      ],
      "metadata": {
        "id": "BetJKF4JhcUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ë‹¨ì–´\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "coefficients = log_clf.coef_[0]\n",
        "\n",
        "# ê¸ì • ì˜í–¥ë ¥ì´ í° ë‹¨ì–´ ìƒìœ„ 10ê°œ\n",
        "top_pos_indices = np.argsort(coefficients)[-10:]\n",
        "print(\"Top positive words:\")\n",
        "for idx in reversed(top_pos_indices):\n",
        "    print(f\"{feature_names[idx]}: {coefficients[idx]:.4f}\")\n",
        "\n",
        "# ë¶€ì • ì˜í–¥ë ¥ì´ í° ë‹¨ì–´ í•˜ìœ„ 10ê°œ\n",
        "top_neg_indices = np.argsort(coefficients)[:10]\n",
        "print(\"\\nTop negative words:\")\n",
        "for idx in top_neg_indices:\n",
        "    print(f\"{feature_names[idx]}: {coefficients[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "yuH8Bh9bg0sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ê¸ì • ë‹¨ì–´\n",
        "top_pos_words = {feature_names[idx]: coefficients[idx] for idx in top_pos_indices}\n",
        "wordcloud_pos = WordCloud(width=600, height=400, background_color='white', colormap='Blues').generate_from_frequencies(top_pos_words)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.imshow(wordcloud_pos, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Top Positive Words\")\n",
        "plt.show()\n",
        "\n",
        "# ë¶€ì • ë‹¨ì–´\n",
        "top_neg_words = {feature_names[idx]: abs(coefficients[idx]) for idx in top_neg_indices}\n",
        "wordcloud_neg = WordCloud(width=600, height=400, background_color='white', colormap='Reds').generate_from_frequencies(top_neg_words)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.imshow(wordcloud_neg, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Top Negative Words\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5nXZ7unE1j6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ê°ì„± ë¶„í¬ ë¶„ì„"
      ],
      "metadata": {
        "id": "BZTrqK4VjTA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì œí’ˆêµ° ë¶„ë¥˜ í•¨ìˆ˜ ì •ì˜\n",
        "def classify_product(text):\n",
        "    text = text.lower()\n",
        "    if \"book\" in text:\n",
        "        return \"Book\"\n",
        "    elif \"camera\" in text or \"lens\" in text:\n",
        "        return \"Electronics\"\n",
        "    elif \"shoe\" in text or \"shirt\" in text:\n",
        "        return \"Clothing\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "train_df[\"product_group\"] = train_df[\"review\"].apply(classify_product)"
      ],
      "metadata": {
        "id": "JkTpkAVEjYhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì œí’ˆêµ°ë³„ ê°ì„± ë¶„í¬ ì‹œê°í™”\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(data=train_df, x=\"product_group\", hue=\"label\")\n",
        "plt.title(\"Sentiment Distribution by Product Group\")\n",
        "plt.xlabel(\"Product Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CH9gab1UjdTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë¦¬ë·° ê¸¸ì´ vs ê°ì„± ê´€ê³„"
      ],
      "metadata": {
        "id": "uZWuBZsWjhHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¦¬ë·° ê¸¸ì´ ì¶”ê°€\n",
        "train_df[\"review_length\"] = train_df[\"review\"].apply(lambda x: len(x.split()))\n",
        "\n",
        "# ì‹œê°í™”\n",
        "sns.boxplot(data=train_df, x=\"label\", y=\"review_length\")\n",
        "plt.title(\"Review Length by Sentiment\")\n",
        "plt.xlabel(\"Sentiment (0=Neg, 1=Pos)\")\n",
        "plt.ylabel(\"Number of Words\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rmkVTEd4jlLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë¶„ë¥˜ê¸° ì„±ëŠ¥ ë¹„êµ(SVM, Naive Bayes, Decision Tree)"
      ],
      "metadata": {
        "id": "N37jnHNajoR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "models = {\n",
        "    \"SVM\": LinearSVC(),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "for name, clf in models.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    preds = clf.predict(X_valid)\n",
        "    acc = accuracy_score(y_valid, preds)\n",
        "    print(f\"{name} Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "fLEPDfGdjyoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}