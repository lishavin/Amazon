{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9Fk3GfVH3a3G",
        "5f8dP3615L2p",
        "MJxACWVIQEF8",
        "pUC59DTrRhYo",
        "W2y-gVxQfCjB",
        "Oz5UM2DLam6X",
        "ixnYmNLXO_LX",
        "uk-Ujcg7QKXr",
        "w7QloeVyS91Z",
        "3263Yn7NfsuV",
        "AlPrlqckhIcY",
        "Ge7uZX1Pj6I4",
        "-XFEIGAOknUo",
        "fXhv4fCbk69M",
        "vbdAaR3JojvY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lishavin/Amazon/blob/main/Glove%2BSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Required Packages\n",
        "\n",
        "- íŒŒì¼ > Driveì— ì‚¬ë³¸ ì €ì¥\n",
        "- ì•„ë˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í›„ session restart í•„ìš” (ëŸ°íƒ€ì„ > ì„¸ì…˜ë‹¤ì‹œì‹œì‘)\n",
        "- drive mount (pretrained weight)"
      ],
      "metadata": {
        "id": "sENqptvHUisT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install numpy==1.26\n",
        "!pip install scipy==1.13.1\n",
        "!pip install gensim\n",
        "!pip install fsspec==2023.4.0 #\"**\" ê²½ë¡œ íŒ¨í„´ í˜¸í™˜ë˜ëŠ” fsspec ì˜›ë‚ ë²„ì „ìœ¼ë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ\n",
        "\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "G9aLq4ZATesT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_ilO50MAuNUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SSipMlOTUml"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "import nltk #í† í°í™”, ë¶ˆìš©ì–´ ì œê±°, í‘œì œì–´ ì¶”ì¶œ ë“±ì„ ìœ„í•œ ìì› ë‹¤ìš´ë¡œë“œ\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Data"
      ],
      "metadata": {
        "id": "joSdnEARUlDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "df = load_dataset(\"fancyzhx/amazon_polarity\")\n",
        "\n",
        "# trainê³¼ testë¥¼ ê°ê° pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
        "train_df = df[\"train\"].select(range(10000)).to_pandas()\n",
        "test_df = df[\"test\"].select(range(5000)).to_pandas()"
      ],
      "metadata": {
        "id": "sLPn7NAZTb6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_df)\n",
        "type(test_df)"
      ],
      "metadata": {
        "id": "52lgghqYIlo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. EDA\n"
      ],
      "metadata": {
        "id": "xSo0G0RAUmbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1 : positive\n",
        "- 0 : negative"
      ],
      "metadata": {
        "id": "9Wx9q1DJSxBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "DGqxjaUDWeAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data type í™•ì¸\n",
        "print(\"Train Dataset DataType\")\n",
        "print(train_df.dtypes)\n",
        "\n",
        "print(\"\\nTest Dataset DataType\")\n",
        "print(test_df.dtypes)"
      ],
      "metadata": {
        "id": "tWzrTcd7WtFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isnull()"
      ],
      "metadata": {
        "id": "OqNMb3m0JFUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ê²°ì¸¡ì¹˜ í™•ì¸\n",
        "print(\"Train Dataset ê²°ì¸¡ì¹˜\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(\"\\nTest Dataset ê²°ì¸¡ì¹˜\")\n",
        "print(test_df.isnull().sum())"
      ],
      "metadata": {
        "id": "1sjzijn4WyBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label ë¹„ìœ¨ í™•ì¸\n",
        "import seaborn as sns\n",
        "sns.countplot(x='label', data= train_df)\n",
        "print(train_df.label.value_counts())"
      ],
      "metadata": {
        "id": "nIOr90GaXJ6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label ë¹„ìœ¨ í™•ì¸\n",
        "sns.countplot(x='label', data= test_df)\n",
        "print(test_df.label.value_counts())"
      ],
      "metadata": {
        "id": "UL-Jz5lUXZZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "2B_vvPTeXzaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Text Cleaning"
      ],
      "metadata": {
        "id": "9Fk3GfVH3a3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">1. ì •ê·œí‘œí˜„ì‹\n",
        "  - HTML íƒœê·¸ ì œê±°\n",
        "  - íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "2. ì†Œë¬¸ì ë³€í™˜\n",
        "3. Stopwords ì œê±°\n",
        "4. Stemming (ì–´ê°„ ì¶”ì¶œ) /Lemmatization (í‘œì œì–´ ì¶”ì¶œ)\n",
        "\n"
      ],
      "metadata": {
        "id": "hSV-0HR3Nbe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Amazon ë°ì´í„°ì— ì ìš©"
      ],
      "metadata": {
        "id": "TmqEjPLQMrub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# titleê³¼ content ì—´ í•©ì¹˜ê¸°\n",
        "train_df[\"review\"] = train_df[\"title\"] + \" \" + train_df[\"content\"]\n",
        "test_df[\"review\"] = test_df[\"title\"] + \" \" + test_df[\"content\"]"
      ],
      "metadata": {
        "id": "E5uczMGdOhco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re #ì •ê·œí‘œí˜„ì‹ regular expression: HTML íƒœê·¸ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì œê±°ì‹œ ì‚¬ìš©\n",
        "import nltk\n",
        "from nltk.corpus import stopwords #nltkì—ì„œ ì œê³µí•˜ëŠ” ë¶ˆìš©ì–´(stopwords) ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ê²ƒ\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer #ì–´ê°„ ì¶”ì¶œ, í‘œì œì–´ ì¶”ì¶œ\n",
        "\n",
        "# ì²˜ìŒ í•œ ë²ˆì€ ë‹¤ìš´ë¡œë“œ í•„ìš”\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_review(review):\n",
        "    # HTML íƒœê·¸ ì œê±°\n",
        "    review = re.sub('<[^>]*>', '', review)\n",
        "    # íŠ¹ìˆ˜ ë¬¸ì ì œê±° (!ëŠ” ë‚¨ê¸°ê¸°)\n",
        "    review = re.sub('[^a-zA-Z0-9 ?]', '', review)\n",
        "    # ì†Œë¬¸ì ë³€í™˜\n",
        "    review = review.lower()\n",
        "    # í† í°í™”\n",
        "    tokens = word_tokenize(review)\n",
        "    # ë¶ˆìš©ì–´ ì œê±°\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # í‘œì œì–´ ì¶”ì¶œ\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "xZmPGCKm31ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ ì ìš©\n",
        "train_df['processed_review'] = train_df['review'].apply(preprocess_review)\n",
        "test_df['processed_review'] = test_df['review'].apply(preprocess_review)"
      ],
      "metadata": {
        "id": "Zx4GvXa-48xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "f-MxJ-AparoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train-Valid Split"
      ],
      "metadata": {
        "id": "W2y-gVxQfCjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80% train, 20% validation\n",
        "data_train, data_valid = train_test_split(\n",
        "    train_df,                  # ì›ë˜ train ë°ì´í„°ì…‹\n",
        "    test_size=0.2,             # 20%ëŠ” validationìœ¼ë¡œ\n",
        "    stratify=train_df['label'], # label ë¹„ìœ¨ ìœ ì§€ (ê¸/ë¶€ì • ê· í˜•)\n",
        "    random_state=42           # ì¬í˜„ì„± (ê°™ì€ split ê²°ê³¼)\n",
        ")\n",
        "\n",
        "data_test = test_df           # í…ŒìŠ¤íŠ¸ëŠ” ì´ë¯¸ ë³„ë„ë¡œ ìˆìŒ"
      ],
      "metadata": {
        "id": "zd5S9b8pfKkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_train)"
      ],
      "metadata": {
        "id": "Mku9ygoRMeXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_valid)"
      ],
      "metadata": {
        "id": "rLmnGrAyMf5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Vectorization + Classifier"
      ],
      "metadata": {
        "id": "Oz5UM2DLam6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Glove\n"
      ],
      "metadata": {
        "id": "w7QloeVyS91Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe ë²¡í„° ë¡œë”©\n",
        "def load_glove_model(glove_file_path):\n",
        "    print(\"Loading GloVe model...\")\n",
        "    glove_model = {}\n",
        "    with open(glove_file_path, encoding=\"utf-8\") as f:\n",
        "        for line in tqdm(f):\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            glove_model[word] = vector\n",
        "    print(f\"{len(glove_model)} words loaded!\")\n",
        "    return glove_model\n",
        "\n",
        "# ë¬¸ì¥ â†’ í‰ê·  ë²¡í„° ë³€í™˜\n",
        "def sentence_to_vector(sentence, glove_model, vector_dim=300):\n",
        "    tokens = sentence.split()\n",
        "    vecs = [glove_model[word] for word in tokens if word in glove_model]\n",
        "    if not vecs:\n",
        "        return np.zeros(vector_dim)\n",
        "    return np.mean(vecs, axis=0)"
      ],
      "metadata": {
        "id": "KOw4ABHdrVN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe ëª¨ë¸ ë¡œë“œ (Stanfordì—ì„œ ì‚¬ì „í•™ìŠµí•œ ëª¨ë¸)\n",
        "glove_path = \"/content/drive/MyDrive/weights/glove.6B.300d.txt\"  # ìœ„ì¹˜ ë§ê²Œ ì§€ì •\n",
        "glove_model = load_glove_model(glove_path)"
      ],
      "metadata": {
        "id": "QAm_W1mGrWRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model['king']"
      ],
      "metadata": {
        "id": "SOvw88CorYEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê° ë¦¬ë·°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
        "tqdm.pandas()\n",
        "X_train_vec = data_train['processed_review'].progress_apply(lambda x: sentence_to_vector(x, glove_model, 300))\n",
        "X_valid_vec = data_valid['processed_review'].progress_apply(lambda x: sentence_to_vector(x, glove_model, 300))\n",
        "X_test_vec = data_test['processed_review'].progress_apply(lambda x: sentence_to_vector(x, glove_model, 300))\n",
        "\n",
        "# numpy arrayë¡œ ë³€í™˜\n",
        "X_train_vec = np.stack(X_train_vec.values)\n",
        "X_valid_vec = np.stack(X_valid_vec.values)\n",
        "X_test_vec = np.stack(X_test_vec.values)\n",
        "\n",
        "y_train = data_train['label'].values\n",
        "y_valid = data_valid['label'].values\n",
        "y_test = data_test['label'].values"
      ],
      "metadata": {
        "id": "gQ_2Gd2drbFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_vec.shape)\n",
        "print(X_train_vec[0][:50])"
      ],
      "metadata": {
        "id": "swm67jHvrc2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Linear SVM"
      ],
      "metadata": {
        "id": "4OAnltifrfVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# SVM ëª¨ë¸ ì •ì˜\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµ\n",
        "svm_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# ì˜ˆì¸¡\n",
        "svm_valid_preds = svm_clf.predict(X_valid_vec)\n",
        "svm_test_preds = svm_clf.predict(X_test_vec)\n",
        "\n",
        "# ì„±ëŠ¥ í‰ê°€\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_valid, svm_valid_preds))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, svm_test_preds))\n",
        "print(\"\\n[Classification Report on Test Set]\\n\", classification_report(y_test, svm_test_preds))"
      ],
      "metadata": {
        "id": "P777x7ehrtCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation 1\n",
        "K-fold ì„ íƒ: íŠœë‹ ì „ baseline ì„±ëŠ¥ì„ í™•ì¸\n"
      ],
      "metadata": {
        "id": "N67aE9ZIopxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_accuracies = []\n",
        "\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(X_train_vec)):\n",
        "    X_tr, X_val = X_train_vec[train_index], X_train_vec[valid_index]\n",
        "    y_tr, y_val = y_train[train_index], y_train[valid_index]\n",
        "\n",
        "    svm_clf = SVC(kernel='linear')\n",
        "    svm_clf.fit(X_tr, y_tr)\n",
        "\n",
        "    val_preds = svm_clf.predict(X_val)\n",
        "    acc = accuracy_score(y_val, val_preds)\n",
        "    fold_accuracies.append(acc)\n",
        "\n",
        "    print(f\"[Fold {fold+1}] Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Average Validation Accuracy (Baseline - KFold 5): {np.mean(fold_accuracies):.4f}\")"
      ],
      "metadata": {
        "id": "DvCGpgTN5lGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "ì´ë¯¸ ëª¨ë¸ ì„±ëŠ¥ì´ ì¢‹ì•„ì„œ ì •ë°€í•œ ì „ìˆ˜ì¡°ì‚¬(Grid)ë³´ë‹¤ëŠ” \të¹ ë¥´ê²Œ ê·¼ì‚¬ ìµœì ê°’ì„ ì°¾ëŠ” ê²Œ ë” ì¤‘ìš”í•´ì„œ Random Searchë¡œ ìµœì  C, penalty ê°’ ì°¾ê¸°\n"
      ],
      "metadata": {
        "id": "LtES2bAOn33U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì •ì˜\n",
        "param_dist = {\n",
        "    'C': uniform(loc=0.01, scale=10),  # ì •ê·œí™” íŒŒë¼ë¯¸í„°\n",
        "    'kernel': ['linear'],              # ì„ í˜• SVM ê³ ì •\n",
        "}\n",
        "\n",
        "# RandomizedSearch ì •ì˜\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=SVC(),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# í•™ìŠµ\n",
        "random_search.fit(X_train_vec, y_train)\n",
        "\n",
        "# ìµœì  ê²°ê³¼ ì¶œë ¥\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", random_search.best_score_)\n",
        "\n",
        "# ìµœì  ëª¨ë¸ ì¶”ì¶œ\n",
        "best_svm = random_search.best_estimator_"
      ],
      "metadata": {
        "id": "aqrBJk2coKjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation 2\n",
        "íŠœë‹ í›„ ìµœì¢… ëª¨ë¸ì„ ì¬ê²€ì¦: RandomizedSearch ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ê°œì„ ëëŠ”ì§€ ë¹„êµ"
      ],
      "metadata": {
        "id": "qoOBEnIhtYnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "tuned_fold_accuracies = []\n",
        "\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(X_train_vec)):\n",
        "    X_tr, X_val = X_train_vec[train_index], X_train_vec[valid_index]\n",
        "    y_tr, y_val = y_train[train_index], y_train[valid_index]\n",
        "\n",
        "    best_svm.fit(X_tr, y_tr)\n",
        "    val_preds = best_svm.predict(X_val)\n",
        "    acc = accuracy_score(y_val, val_preds)\n",
        "    tuned_fold_accuracies.append(acc)\n",
        "\n",
        "    print(f\"[Fold {fold+1}] Validation Accuracy (Tuned): {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nâœ… Average Validation Accuracy (Tuned - KFold 5): {np.mean(tuned_fold_accuracies):.4f}\")"
      ],
      "metadata": {
        "id": "AWhbRfeyu0Nf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}